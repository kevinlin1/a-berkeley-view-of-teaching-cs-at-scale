\chapter{Automation}
\label{chapter:automation}

For decades, CS courses at UC Berkeley have used command-line interface autograding utilities such as the \texttt{grading} package. This package allows students to submit files (e.g. programming assignments) to the instructor's UNIX account for grading. Autograding is available through makefiles defined by the instructor for each assignment, and feedback can be automatically emailed to students once the autograder finishes execution. More recent autograding solutions utilize container technology to improve reliability and web frontends to improve the user experience.

In large CS courses, much of the feedback on program correctness is provided by autograders. This has both practical and pedagogical benefits as it reduces the workload of grading student work while supporting students to make progress through independent debugging. These autograding solutions have traditionally been supported by a single computing server provided by the department dedicated to supporting the grading needs of each course. However, when hundreds of students request autograding resources (often in the hours before assignment deadlines), student submissions are placed in a long grading queue. This is an especially important consideration because many UC Berkeley CS courses rely on automated feedback to provide assistance to a large number of students, so a long grading queue is an educational denial of service.

Furthermore, large courses often require additional assistance to manage the flow of information, students, and staff. Beyond the work of grading and returning graded work to students assisted by autograding software, there is a need to improve organization of students and teachers in office hours, and to distribute announcements, assignments, and learning materials to students as the number of students grows beyond traditional classroom capacities. As with autograding, student-facing infrastructure needs to scale as student demand can reach thousands of requests per hour, course rosters can grow beyond 1,000 students long, and there can be tens of thousands of forum posts per semester. These challenges, which can easily become immense administrative burdens at very large scale, have led to the development of specialized software to reduce the administrative overhead of running large courses. For instance, administering exams to over 1,600 students spread across as many as 20 rooms on campus has required the development of specialized software as well as more flexible, student-friendly policies and procedures to enable efficient support of hundreds of exam exceptions and accommodations each semester.

\section{Grading and Feedback}

Two of the most well-known grading and feedback web apps developed at UC Berkeley are Gradescope and OK. These two web apps stand out as particularly unique in how they have enabled new pedagogies and practices.

\subsection{Gradescope}

In Spring 2012, Pandagrader was conceived by the course staff teaching Introduction to Artificial Intelligence to streamline the process of grading paper exams. As instructors from other courses and institutions rapidly adopted the tool, in 2014, Pandagrader was incorporated as Gradescope.\footnote{\url{https://gradescope.com}}

The exam grading workflow begins with scanning and uploading exams to Gradescope. For very large courses with two, high-throughput copy machines, scanning all of the exams can take between 1--3 hours but yields significant time savings during the actual grading through Gradescope's fast online grading interface. Exam scores can then be turned around to students without returning physical papers: students view their graded exams online and see exactly which rubric items were applied. All grading and regrade requests are done over Gradescope's web interface which normally hides the identity of the student and the instructor from each other to minimize bias. The grading process is simpler for homework assignments as students submit assignments online themselves.

By moving the grading workflow online, course staff can increase their grading efficiency. Instead of shuffling papers, course staff assign rubric items to student submissions with only a single click or keystroke, and advance to the next submission with just another click or keystroke. Grading assignments according to an instructor-defined rubric improves transparency to students, helps ensure consistency between multiple graders, and makes it easy for the instructor to adjust point allocations or rubric item description post-hoc. Certain types of questions such as multiple choice or short answer blanks can be graded even more quickly with machine learning-assisted answer grouping where student submissions are automatically categorized into unique answer groups, allowing instructors to grade each group rather than each individual submission. Grading progress and statistics are computed on-the-fly, providing insight into aggregate and individual student success data. Gradescope's distributed grading system is also helpful for grading weekly problem sets. Students submit their work directly to Gradescope. Submissions can be graded online, enabling students to receive feedback the same week or even the day after submission depending on the length of the assignment and the amount of grading support.

The fast turnaround time also makes Gradescope a platform for providing formative feedback to students in two introductory CS courses at UC Berkeley. In these courses, students take a short paper quiz in their discussion or lab section during the day. After the section ends, the section TA scans their quizzes and uploads them to Gradescope. In the evening or later in the week, the course instructor and a handful of graders then grades all of the quizzes on Gradescope, identifying the entire class's common misconceptions, and returns personalized feedback to students via email. In end-of-semester course evaluations, students appreciated the additional feedback and found the system as a whole beneficial for their learning. Instructors and section TAs gain a detailed view of student performance previously unavailable in large courses while minimizing additional grading responsibilities. Using the misconceptions collected through weekly quizzes, instructors have released additional, targeted tutorials, practice materials, and study guides to help students improve before taking a high-stakes exams.

In 2017, Gradescope added support for autograding programming assignments. Gradescope's autograding platform is designed on top of container virtualization technology, allowing servers to start new autograder instances as needed and spread load across multiple machines. This ensures that students do not need to wait in a queue: new autograder instances can be provisioned as soon as students submit their assignments and, in times of greatest demand, servers can be dynamically added to the computing resource pool. This approach benefits instructors as they have full control over their choice of grading language and environment in the container. But it also presents a new cost as time needs to be invested in designing small programs or scripts which produce outputs following a specific autograder specification. Manual grading is also possible with grading rubrics and inline comments through a workflow similar to that of the homework and exam assignment types. The recent integration of a system for detecting software similarity has the potential to additionally simplify the workflow for identifying and understanding cases of over-collaboration.

\subsection{OK}

OK\footnote{\url{https://okpy.org}} is an online autograding platform developed by the course staff teaching introductory computer science at UC Berkeley in Fall 2014. Its primary users are introductory computer science and data science courses at UC Berkeley, each regularly serving enrollments 500--1,500 students per semester. The OK platform provides an alternative to Gradescope's autograding for programming assignments, though its approach favors much deeper integration with the course. In addition to a web interface for students to submit programming assignments, OK provides a Python client that boasts three key features over Gradescope's server-side grading.

\subsubsection{Student-Side Autograding}

With student-side autograding, students can run a suite of tests on their computer at any time, providing instantaneous code correctness feedback without needing to formally submit their assignment. These test suites are written in Python doctest format, a format students are familiar with from their practice using with the interactive Python shell, which makes the tests and results easier for students to interpret than traditional unit tests. Complex integration tests can be written in separate files that are seamlessly integrated into the system \cite{Sridhara:2016}, appearing only after the submission passes targeted doctests. OK presents a simple interface to this feature so that students can autograde any part of the assignment without memorizing a complicated command.

Student-side autograding has resulted in a qualitative change in the way students approach problems as they now often use the autograder as part of a continuous edit-test feedback loop. For every change to a piece of code, students will re-run the autograder to see how the change affects the program. The immediate feedback then informs future planning, implementation, and debugging behaviors. This real-time feedback reduces frustration and builds student confidence by helping them make progress where they would normally get stuck. But there is an inherent risk to this approach. While students may be able to make more progress with the help of the student-side autograders, they may also grow more dependent on the feedback. Several introductory CS courses have experimented with velocity limiting which limits autograder usage (either student-side or server-side) to a certain number of attempts which resets after a fixed amount of time. Students are encouraged to instead try out different debugging and problem-solving techniques such as developing their own examples and running through it on paper to check their understanding. This automated feedback has become an important cornerstone of introductory CS at UC Berkeley.

\subsubsection{Test Unlocking}

One of the risks of student-side autograding is that students can access tests without thinking through the problem which can lead to a dependency on this development cycle. With test unlocking, the expected output of each doctest is stored as an encrypted string until it is successfully unlocked. Before students can run the student-side autograder, they must explore the problem by unlocking each automated test and determining the expected outputs by hand. Test unlocking reduced the number of conceptual questions (misunderstandings or clarifications of the problem), allowing instructors to spend more time assisting students with more involved questions: unlocking the tests helped students better understand the problem specifications and ``work through the thought process'' \cite{Basu:2015}. This result has been cited to provide metacognitive scaffolding \cite{Prather:2019}. Students later have the opportunity to write and run their own tests, reinforcing program understanding.

\subsubsection{Automatic Backups}

Each time the student-side autograder is invoked, student work is automatically backed up to the OK server along with metadata on their current progress. This submission and metadata collected by the automatic backup mechanism helps instructors answer questions about student learning. Together with the code backup, the OK client program records analytics such as the current question the student is working on, number of times the student-side autograder has been invoked thus far, and correctness. In addition, because the student-side autograder is run as part of a continuous edit-test feedback loop, the OK server aggregates a large number of intermediary student submissions. Instructors can then track student progress in aggregate and at the individual level to a high degree of detail. Frequent intermediary code snapshots have also benefited the research community as it has generated a massive dataset for education researchers to model student learning and performance \cite{Piech:2012, Wang:2017, Liao:2019}, deploy learning interventions at scale \cite{Sridhara:2016, Phothilimthana:2017}, understand excessive collaboration \cite{Yan:2018}, and propagate feedback at scale \cite{Yan:2019, Glassman:2015}. Students have the option to disable automatic backups and metadata collection but still run the student-side autograder by running the OK client with the appropriate command-line arguments.

\section{Managing Student Learning}

Beyond grading and feedback, many other components of a traditional lecture-format course such as office hours, exam scheduling, and discussion and lab scheduling are more difficult to organize at scale as they often require coordinating a large number of students.

\subsection{Office Hours}

Drop-in office hours in computer science courses have historically been organized as single-instructor events. The instructor announces their office hours time and place, and students visit office hours to interact with the instructor, often asking questions about assignments, concepts from lecture, as well as topics not directly related to the course such as undergraduate research opportunities and their own personal interests. However, as enrollments have increased, this model has proven increasingly difficult to scale especially as the number of homework questions has grown with the number of students. Questions left unanswered during discussion and lab section are clarified in office hours, leading to more demand for teaching assistance in office hours.

Until recently, this model was also true for teaching assistant office hours. In TA office hours, students mostly ask questions related to their assignments. TA office hours often used the same model with one TA walking around and answering each question. As more students arrive at office hours and the number of students waiting increases, students sign themselves up on a queue to reserve their place in line, ensuring that students who have waited the longest get helped next. While this model works up until about 10 students in office hours, as more students join the queue, there is pressure on the course staff to resolve all of the questions on the queue or risk ending office hours with several disappointed students. Overcrowded office hours are one of the highest-visibility issues for large lecture courses and a common source of student complaints.

Double, triple, and quadruple-staffing office hours has helped to alleviate supply and demand mismatches. Instead of office hours simply being the responsibility of a single TA, multiple TAs staff each office hour to meet student demand. However, one of the side effects of increasing the supply of TAs is that office hours can quickly grow out of control and unmanageable.  Students may jump ahead in the queue and get help from multiple different instructors which not only takes teaching resources away from other students, but can also potentially shortcut the student's learning. Locating the next student in the queue is often a mess as instructors shout names across the room and need to navigate a maze of students to reach the next student on the queue.

Software can be used to streamline drop-in office hours by matching TAs to the next unresolved student help request through an online first-come, first-served queue. The Office Hours Queue web app, developed by undergraduate teaching assistants in the EECS Department on top of the OK platform, presents an interface for students and TAs to interact with the queue in much the same workflow as before. Students login and request help through the web app, specifying the assignment, question, location, and a brief description of the help request. TAs access the web app on their smartphones to view students on the queue. When a TA chooses to help the next student, the system marks the request as currently being helped, displays the name of the student to the TAs, and sends a notification to the student's device. The TA then works with the student to help resolve their question and make progress on their assignment. Finally, the TA marks the help request as resolved, or, if the help request was not resolved, returns the help request back to its original place on the queue. Similar systems have been deployed at peer institutions \cite{Smith:2017, MacWilliam:2013}. An introductory computer science student commented that,
\begin{quote}
    The additional office hour staff and expanded hours during [the final project] was great! I remember having to wait 2--3 hours and not being able to receive help for projects and assignments at the beginning of the school year because office hours were so packed and there wasn't really an orderly way to identify students who needed help.
\end{quote}
Other institutions have taken this model even further by moving office hours into highly-frequented, open spaces on campus for the convenience of students and course staff \cite{MacWilliam:2013}. In spite of these improvements, office hours remains oversubscribed, and finding spaces large enough and flexible enough for office hours remains a challenge.

This model for drop-in office hours treats student help requests as discrete, individual tickets. However, other models for office hours, such as The Tao of TALC, can also be highly productive. Instead of directly assisting students, The Tao of TALC encourages instructors to be guides wherein ``students drive as much as possible'' and the instructor acts ``more as a facilitator between the confused student and their `peer instructors'{}'' \cite{Astrachan:2007}. This has been used successfully both by instructors of small-group office hours and by TAs running large-group office hours. This group learning model has informed one of the features of the office hours queue which enables a single instructor to help a group of students working on the same question all at once.

The use of online office hours queues has generated a large amount of data on office hours usage patterns \cite{Smith:2017}. Data dashboards for the office hours queue give instructors a view of which assignments students are asking questions about and when. Using this information, course staff have been able to shift resources and staff the most heavily-impacted office hours with more TAs where they were needed the most, further reducing student wait times. We have found that the majority of office hours are utilized by a very small minority of students: in a class of over 1,400 students, 50 students asked half of all the questions in office hours that semester.

\subsection{Online Course Delivery}

Course delivery is the process of offering a course to students. Office hours, lab, and discussion section are but a few ways students learn in large lecture courses. CS courses at UC Berkeley emphasize solving problems which can occur through lecture, office hours, lab, and discussion section, but is often further emphasized in homework assignments and programming projects which combine multiple concepts presented in the course. As enrollments for several lower-division and upper-division CS courses regularly exceed the capacity of the largest lecture halls on campus, lectures in most CS courses are now webcasted. Making attendance at lecture, lab, and discussion section optional has had profound impacts on the way large lecture courses are run and has required special attention from the instructor.

In webcasted courses, it is much easier for students to fall behind and out-of-sync with content from lecture. Course policies need to be designed such that webcasted courses demand enough attention and consistent effort from students so that they stay on track. The assignment of frequent, small quizzes checks that students are keeping up with lecture. At UC Berkeley, simulcasts are not offered so live lecture recordings are often delayed by at least two hours. Nonetheless, even with interventions to keep students at pace with the course, it is often the case that many students will be at least a few days behind schedule at various times during the semester due to commitments from other courses taking priority.

In order to keep asynchronous classes up-to-speed, most large CS courses at UC Berkeley deliver their content via course websites or online discussion forums such as Piazza. The largest introductory CS courses update the front page of their website frequently with announcements so that students treat it as the definitive authority for the course, reducing student questions about assignments, deadlines, and exams. All materials in the course, including lecture videos, lecture notes, assignments, and readings, are always posted online which enables students to choose how they would like to learn: either with a long-distance learning model, or by attending activities in a more traditional classroom setting.

Experienced TAs have also devised workarounds to make Piazza more robust for large courses.
\begin{quotation}
    Piazza will routinely be flooded with questions, especially near exams. Managing Piazza effectively does not necessarily mean keeping up with this flood of questions; it means directing the flow of questions efficiently so that students with similar questions can easily find previous students' questions and avoid asking repeat questions.

    Each week, keep a pinned Piazza thread for each separate homework question. This is the most important rule to follow, as homework questions will comprise the majority of all Piazza questions. Also, we recommend creating separate threads for each lecture note and discussion. Once these threads are in place, aggressively mark student questions as duplicates and move them to the appropriate threads so that all similar questions are in one place.

    To help students find these various threads, create a pinned master index which contains links to all of the above threads, as well as threads containing homework grade distributions, TA resources, weekly posts, events (guerrilla sections, exam review sessions, etc.), and other announcements. The index requires maintenance, so either consciously assign the management of the index as a TA duty, or make it understood within the team that each person who posts new material on the Piazza is also responsible for adding a link in the index.
\end{quotation}

Automation has also benefited online course delivery by streamlining administrative processes. In order to facilitate rapid updates, course websites are deployed using static website generators such as Jekyll for easy updating by course staff. Small programming questions and math problems are checked into a version-controlled question bank consisting of every question ever developed in the course so that developing a homework assignment can be as simple as specifying which questions to include and running a makefile to deploy the assignment to the website. TAs have even developed scripts to automatically extract screenshots and compose Piazza threads for each question. GitHub Classroom allows instructors to easily provision private GitHub repositories for student work and automatically configure instructor permissions. Some courses which require more advanced configuration have developed their own custom web scripts for provisioning private repositories. By automating these tasks, more time can be spent supporting student learning.

\subsection{Exam Administration}

Administering exams at scale is a particular pain point for large courses. One of the reasons for this is the number of students who need to be organized together, on-campus, at one time. In the largest introductory CS courses, it is common for exams to be simultaneously proctored across all of the largest lecture halls on campus, as well as spread across a dozen smaller classrooms. Traditionally, the course staff assigns students to exam rooms based on name or student identification number ranges. While, in theory, this system should allow the staff to distribute students however they like, large courses are hampered by the number of students requesting exam accommodations. In the largest introductory CS courses, there can be as many as 100 students requesting accommodations which makes coordinating exam seating a challenge of its own.

Undergraduate teaching assistants in the EECS Department developed the Seating web app to solve this problem. The exam seating tool allows instructors to design seating charts, populate them with students based on their individual preferences, and send personalized emails to students with their particular exam room and exact seat location. Seating charts can be specified to assign students to every-other seat, or to skip entire rows of seats to make it easier for TAs to answer questions from students in the middle of a long row in a packed lecture hall.

Adopting this software has enabled large courses to provide better accommodations to all students, both students with and without documented disabilities. Students who are left-handed, or those who simply prefer to sit on a particular side of the room, near the aisle, or near the front can mark their preferences in a form. The Seating app will then take those preferences into account when randomly assigning a seat to the student. Since seats are assigned, it is easy to account for missing students and produces a paper trail making it more difficult for students to cheat on exams. Before, during, and after the exam, instructors can lookup any student on the seating chart and immediately identify adjacent students. When used with Gradescope, the Seating app makes it easier to compare suspicious student work against other students sitting in their vicinity.

Peer institutions have developed specialized computer-based testing facilities \cite{Nip:2018, West:2016} and restricted computing environments for test-taking \cite{Piech:2018} which have demonstrated benefits for student learning while reducing the amount of course staff resources spent administering exams.